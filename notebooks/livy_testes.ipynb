{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json, pprint, requests, textwrap\n",
    "import findspark\n",
    "\n",
    "import json, pprint, requests, textwrap\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "os.environ[\"JAVA_HOME\"] = r'/lib/jvm/java-11-openjdk-amd64'\n",
    "os.environ[\"SPARK_HOME\"] = r\"/home/guilherme/docker-livy/code/apps/spark\"\n",
    "\n",
    "os.environ[\"HADOOP_OPTS\"]=\"$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native\"\n",
    "os.environ[\"LIVY_CONF_DIR\"]=r\"/opt/livy\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.getenv('user')\n",
    "password = os.getenv('password')\n",
    "tabela = os.getenv('table')\n",
    "url = os.getenv('URI_BI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def reader_csv(spark: SparkSession) -> None:\n",
    "    df = spark.read.option(\"inferSchema\",True) \\\n",
    "                 .options(header='True', inferSchema='True', delimiter=';') \\\n",
    "                .csv(r\"/home/guilherme/docker-livy/code/apps/SEG_TIM_TREINAR_MODELO_V2_ABR_31-05_1_1.CSV\")\n",
    "    \n",
    "\n",
    "    print(df.show(10))\n",
    "\n",
    "    df.createOrReplaceTempView(\"ProdutosView\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executa_consulta_tabela1(spark: SparkSession) -> None:\n",
    "    \"\"\"Faz a consulta da tabela e salva em um Parquet na pasta spark-wareHouse\"\"\"\n",
    "\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"dbtable\", tabela) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password)\\\n",
    "        .load()\n",
    "    \n",
    "\n",
    "    jdbcDF.write.mode('overwrite') \\\n",
    "         .saveAsTable(tabela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.master\",\"local[*]\")\n",
    "    conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\",\"true\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "    conf.set(\"spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.statistics.size.autoUpdate.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\",\"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "    spark = SparkSession.builder\\\n",
    "            .config(conf=conf)\\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\",True) \\\n",
    "                 .options(header='True', inferSchema='True', delimiter=';') \\\n",
    "                .csv(r\"/home/guilherme/docker-livy/code/apps/SEG_TIM_TREINAR_MODELO_V2_ABR_31-05_1_1.CSV\")\n",
    "\n",
    "df.createOrReplaceTempView(\"ProdutosView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+-----+-------+----+----+----------------+-------+--------------------+\n",
      "|DT_REFERENCIA_TIM|       FONE|QTD_DISCADOS|QTD_ALO|VENDAS|CAMPANHA_CLARO|DT_REFERENCIA_CLARO|PLANO|SEGREAL|   X|   Y|RENDA_PER_CAPITA|DDFAIXA|NM_SUBCLUSTER_DEPARA|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+-----+-------+----+----+----------------+-------+--------------------+\n",
      "|       2023-04-01|11910086543|           1|      1|     0|          null|               null| null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910221828|           7|      4|     0|     MADURAS_A|         2023-04-01|  P�S|      2|   F|   B|             555|      B|                 A42|\n",
      "|       2023-04-01|11910205181|           3|      1|     0|          null|               null| null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910071709|           3|      2|     0|          null|               null| null|   null|null|null|            null|      B|                null|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+-----+-------+----+----+----------------+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+--------+-------+----+----+----------------+-------+--------------------+\n",
      "|DT_REFERENCIA_TIM|       FONE|QTD_DISCADOS|QTD_ALO|VENDAS|CAMPANHA_CLARO|DT_REFERENCIA_CLARO|   PLANO|SEGREAL|   X|   Y|RENDA_PER_CAPITA|DDFAIXA|NM_SUBCLUSTER_DEPARA|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+--------+-------+----+----+----------------+-------+--------------------+\n",
      "|       2023-04-01|11910086543|           1|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910221828|           7|      4|     0|     MADURAS_A|         2023-04-01|     P�S|      2|   F|   B|             555|      B|                 A42|\n",
      "|       2023-04-01|11910205181|           3|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910071709|           3|      2|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910239544|           1|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910239728|           1|      1|     0|     MADURAS_A|         2023-03-01|CONTROLE|      1|   F|   A|             555|      B|                  A4|\n",
      "|       2023-04-01|11910001000|           8|      3|     0|MADURAS_CC_>90|         2022-05-01|     PRE|      C|   C|   C|            1001|      B|                  A4|\n",
      "|       2023-04-01|11910221310|           1|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910217745|           2|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11325569291|           5|      2|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+--------+-------+----+----+----------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "reader_csv(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o310.load.\n: java.lang.NullPointerException\n\tat java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat java.base/java.util.Properties.put(Properties.java:1340)\n\tat java.base/java.util.Properties.setProperty(Properties.java:228)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$asProperties$1(JDBCOptions.scala:54)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:54)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m executa_consulta_tabela1(spark)\n",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m, in \u001b[0;36mexecuta_consulta_tabela1\u001b[0;34m(spark)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecuta_consulta_tabela1\u001b[39m(spark: SparkSession) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Faz a consulta da tabela e salva em um Parquet na pasta spark-wareHouse\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     jdbcDF \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread \\\n\u001b[1;32m      5\u001b[0m         \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      6\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m, url) \\\n\u001b[1;32m      7\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m'\u001b[39;49m\u001b[39mdriver\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m      8\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      9\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39minferSchema\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     10\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdbtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, tabela) \\\n\u001b[1;32m     11\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, user) \\\n\u001b[1;32m     12\u001b[0m         \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mpassword\u001b[39;49m\u001b[39m\"\u001b[39;49m, password)\\\n\u001b[1;32m     13\u001b[0m         \u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     16\u001b[0m     jdbcDF\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mmode(\u001b[39m'\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m          \u001b[39m.\u001b[39msaveAsTable(tabela)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o310.load.\n: java.lang.NullPointerException\n\tat java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat java.base/java.util.Properties.put(Properties.java:1340)\n\tat java.base/java.util.Properties.setProperty(Properties.java:228)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$asProperties$1(JDBCOptions.scala:54)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:54)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "executa_consulta_tabela1(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'starting', 'id': 0, 'kind': 'spark'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "host = 'http://localhost:8998'\n",
    "data = {'kind': 'spark'}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'state': u'starting', u'id': 0, u'kind': u'spark'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'idle', 'id': 0, 'kind': 'spark'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_url = host + r.headers['location']\n",
    "r = requests.get(session_url, headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'state': u'idle', u'id': 0, u'kind': u'spark'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': None, 'state': 'running', 'id': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements_url = session_url + '/statements'\n",
    "data = {'code': '1 + 1'}\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'output': None, u'state': u'running', u'id': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions fetched starting  0\n",
      "Number of session fetched 2\n",
      "The session 0 has a state idle \n",
      "The session 1 has a state idle \n"
     ]
    }
   ],
   "source": [
    "#Change host per your confirgration                               \n",
    "host = \"http://localhost:8998\"\n",
    "\n",
    "#Livy interface as per https://livy.incubator.apache.org/docs/latest/rest-api.html#session\n",
    "\n",
    "# Construct Request - Get a list of current spark sessions.\n",
    "sessions_url = host + \"/sessions\"\n",
    "\n",
    "# Common headers for all requests.\n",
    "# Auth header\n",
    "auth = HTTPBasicAuth(\"***\", \"***\")\n",
    "# Content Type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "data = {\n",
    "        'from': 0, \n",
    "        'size': 10\n",
    "       }\n",
    "r = requests.get(sessions_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "\n",
    "response_body = r.json()\n",
    "\n",
    "print(\"Sessions fetched starting \", response_body['from'])\n",
    "print(\"Number of session fetched\", response_body['total'])\n",
    "session_list = response_body['sessions']\n",
    "\n",
    "for session in session_list:\n",
    "    print(\"The session {0} has a state {1} \".format(session['id'],session['state']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session 2 created. Current state is starting\n"
     ]
    }
   ],
   "source": [
    "#Create a new Spark session\n",
    "data = {\n",
    "    'kind':'pyspark'\n",
    "}\n",
    "\n",
    "r = requests.post(sessions_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "response_body = r.json()\n",
    "\n",
    "session_id = response_body['id']\n",
    "session_state = response_body['state']\n",
    "print(\"Spark session {0} created. Current state is {1}\".format(session_id,session_state))\n",
    "created_session_url = r.headers['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_session_url http://localhost:8998/sessions/2\n",
      "{'appId': None,\n",
      " 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None},\n",
      " 'id': 2,\n",
      " 'kind': 'pyspark',\n",
      " 'log': ['23/06/21 04:58:30 INFO Utils: Successfully started service '\n",
      "         \"'org.apache.spark.network.netty.NettyBlockTransferService' on port \"\n",
      "         '40609.',\n",
      "         '23/06/21 04:58:30 INFO NettyBlockTransferService: Server created on '\n",
      "         '45dab089587a:40609',\n",
      "         '23/06/21 04:58:30 INFO BlockManager: Using '\n",
      "         'org.apache.spark.storage.RandomBlockReplicationPolicy for block '\n",
      "         'replication policy',\n",
      "         '23/06/21 04:58:30 INFO BlockManagerMaster: Registering BlockManager '\n",
      "         'BlockManagerId(driver, 45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO BlockManagerMasterEndpoint: Registering block '\n",
      "         'manager 45dab089587a:40609 with 366.3 MB RAM, BlockManagerId(driver, '\n",
      "         '45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO BlockManagerMaster: Registered BlockManager '\n",
      "         'BlockManagerId(driver, 45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO BlockManager: Initialized BlockManager: '\n",
      "         'BlockManagerId(driver, 45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO StandaloneSchedulerBackend: SchedulerBackend '\n",
      "         'is ready for scheduling beginning after reached '\n",
      "         'minRegisteredResourcesRatio: 0.0',\n",
      "         '23/06/21 04:58:30 INFO SparkEntries: Spark context finished '\n",
      "         'initialization in 695ms',\n",
      "         '23/06/21 04:58:30 INFO SparkEntries: Created Spark session.'],\n",
      " 'name': None,\n",
      " 'owner': None,\n",
      " 'proxyUser': None,\n",
      " 'state': 'idle'}\n"
     ]
    }
   ],
   "source": [
    "#Query information about the session we just created\n",
    "this_session_url = host + created_session_url\n",
    "print(\"this_session_url\", this_session_url)\n",
    "\n",
    "r = requests.get(this_session_url, headers=headers,auth=auth, verify=False)\n",
    "pprint.pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'http://localhost:8998/sessions/2/statements'\n",
      "Respone status code 201\n",
      "Response received is  {'id': 2, 'code': '11+11', 'state': 'waiting', 'output': None, 'progress': 0.0, 'started': 0, 'completed': 0}\n",
      "Poll URI is  /sessions/2/statements/2\n"
     ]
    }
   ],
   "source": [
    "#Execute code interactively in this session using session/<ID>/statements interface\n",
    "\n",
    "statements_url = this_session_url + \"/statements\"\n",
    "pprint.pprint(statements_url)\n",
    "\n",
    "data = {\n",
    "  'code': \"11+11\"\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "print(\"Respone status code\" ,r.status_code )\n",
    "print(\"Response received is \",r.json())\n",
    "print(\"Poll URI is \",r.headers['location'])\n",
    "#response_body = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monitoring url is  http://localhost:8998/sessions/2/statements/2\n",
      "Response status is  200\n",
      "{'code': '11+11',\n",
      " 'completed': 1687323629800,\n",
      " 'id': 2,\n",
      " 'output': {'data': {'text/plain': '22'}, 'execution_count': 2, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'started': 1687323629799,\n",
      " 'state': 'available'}\n",
      "Response received is  None\n"
     ]
    }
   ],
   "source": [
    "# Wait a while before executing this cell. Spark session start up takes time to run your code.\n",
    "specific_statement = host + r.headers['location']\n",
    "print(\"monitoring url is \", specific_statement)\n",
    "\n",
    "r = requests.get(specific_statement, headers=headers, auth=auth, verify=False)\n",
    "print(\"Response status is \",r.status_code)\n",
    "print(\"Response received is \", pprint.pprint(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_url http://localhost:8998/batches\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'location'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m'\u001b[39m\u001b[39m/home/guilherme/docker-livy/teste.py\u001b[39m\u001b[39m'\u001b[39m    \n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(batch_url, data\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mdumps(data), headers\u001b[39m=\u001b[39mheaders, auth\u001b[39m=\u001b[39mauth, verify\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m returned_batch_url \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mheaders[\u001b[39m'\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRespone status code\u001b[39m\u001b[39m\"\u001b[39m , r\u001b[39m.\u001b[39mstatus_code)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPoll URI is \u001b[39m\u001b[39m\"\u001b[39m,returned_batch_url )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/structures.py:52\u001b[0m, in \u001b[0;36mCaseInsensitiveDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store[key\u001b[39m.\u001b[39;49mlower()][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'location'"
     ]
    }
   ],
   "source": [
    "#Execute code in batch\n",
    "#The following uses pi.py from spark source. Please get that file and transer to HDFS /jar folder.\n",
    "batch_url = host + \"/batches\"\n",
    "print(\"batch_url\", batch_url)\n",
    "\n",
    "data = {\n",
    "    'file' : '/home/guilherme/docker-livy/teste.py'    \n",
    "}\n",
    "\n",
    "r = requests.post(batch_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "returned_batch_url = r.headers\n",
    "print(\"Respone status code\" , r.status_code)\n",
    "print(\"Poll URI is \",returned_batch_url )\n",
    "print(\"Response is \", pprint.pprint(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returned_batch_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Check results of executed code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m specific_batch \u001b[39m=\u001b[39m host \u001b[39m+\u001b[39m returned_batch_url\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mspecific batch request \u001b[39m\u001b[39m\"\u001b[39m,specific_batch)\n\u001b[1;32m      5\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(specific_batch,headers\u001b[39m=\u001b[39mheaders, auth\u001b[39m=\u001b[39mauth, verify \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'returned_batch_url' is not defined"
     ]
    }
   ],
   "source": [
    "#Check results of executed code\n",
    "specific_batch = host + returned_batch_url\n",
    "print(\"specific batch request \",specific_batch)\n",
    "\n",
    "r = requests.get(specific_batch,headers=headers, auth=auth, verify = False)\n",
    "print(\"Response status is \",r.status_code)\n",
    "print(\"Response received is \", pprint.pprint(r.json()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
