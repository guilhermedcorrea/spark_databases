{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json, pprint, requests, textwrap\n",
    "import findspark\n",
    "\n",
    "import json, pprint, requests, textwrap\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "os.environ[\"JAVA_HOME\"] = r'/lib/jvm/java-11-openjdk-amd64'\n",
    "os.environ[\"SPARK_HOME\"] = r\"/home/guilherme/docker-livy/code/apps/spark\"\n",
    "\n",
    "os.environ[\"HADOOP_OPTS\"]=\"$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native\"\n",
    "os.environ[\"LIVY_CONF_DIR\"]=r\"/opt/livy\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.getenv('user')\n",
    "password = os.getenv('password')\n",
    "tabela = os.getenv('table')\n",
    "url = os.getenv('URI_BI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executa_consulta_tabela_sql(spark: SparkSession) -> None:\n",
    "    \"\"\"Faz a consulta da tabela e salva em um Parquet na pasta spark-wareHouse\"\"\"\n",
    "\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"dbtable\", tabela) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password)\\\n",
    "        .load()\n",
    "    \n",
    "\n",
    "    jdbcDF.write.mode('overwrite') \\\n",
    "         .saveAsTable(tabela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.master\",\"local[*]\")\n",
    "    conf.set(\"spark.executor.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\",\"true\")\n",
    "    conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "    conf.set(\"spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.statistics.size.autoUpdate.enabled\",\"true\")\n",
    "    conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\",\"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "    conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "    spark = SparkSession.builder\\\n",
    "            .config(conf=conf)\\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\",True) \\\n",
    "                 .options(header='True', inferSchema='True', delimiter=';') \\\n",
    "                .csv(r\"/home/guilherme/docker-livy/code/apps/SEG_TIM_TREINAR_MODELO_V2_ABR_31-05_1_1.CSV\")\n",
    "\n",
    "df.createOrReplaceTempView(\"ProdutosView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+-----+-------+----+----+----------------+-------+--------------------+\n",
      "|DT_REFERENCIA_TIM|       FONE|QTD_DISCADOS|QTD_ALO|VENDAS|CAMPANHA_CLARO|DT_REFERENCIA_CLARO|PLANO|SEGREAL|   X|   Y|RENDA_PER_CAPITA|DDFAIXA|NM_SUBCLUSTER_DEPARA|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+-----+-------+----+----+----------------+-------+--------------------+\n",
      "|       2023-04-01|11910086543|           1|      1|     0|          null|               null| null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910221828|           7|      4|     0|     MADURAS_A|         2023-04-01|  P�S|      2|   F|   B|             555|      B|                 A42|\n",
      "|       2023-04-01|11910205181|           3|      1|     0|          null|               null| null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910071709|           3|      2|     0|          null|               null| null|   null|null|null|            null|      B|                null|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+-----+-------+----+----+----------------+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+--------+-------+----+----+----------------+-------+--------------------+\n",
      "|DT_REFERENCIA_TIM|       FONE|QTD_DISCADOS|QTD_ALO|VENDAS|CAMPANHA_CLARO|DT_REFERENCIA_CLARO|   PLANO|SEGREAL|   X|   Y|RENDA_PER_CAPITA|DDFAIXA|NM_SUBCLUSTER_DEPARA|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+--------+-------+----+----+----------------+-------+--------------------+\n",
      "|       2023-04-01|11910086543|           1|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910221828|           7|      4|     0|     MADURAS_A|         2023-04-01|     P�S|      2|   F|   B|             555|      B|                 A42|\n",
      "|       2023-04-01|11910205181|           3|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910071709|           3|      2|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910239544|           1|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910239728|           1|      1|     0|     MADURAS_A|         2023-03-01|CONTROLE|      1|   F|   A|             555|      B|                  A4|\n",
      "|       2023-04-01|11910001000|           8|      3|     0|MADURAS_CC_>90|         2022-05-01|     PRE|      C|   C|   C|            1001|      B|                  A4|\n",
      "|       2023-04-01|11910221310|           1|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11910217745|           2|      1|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "|       2023-04-01|11325569291|           5|      2|     0|          null|               null|    null|   null|null|null|            null|      B|                null|\n",
      "+-----------------+-----------+------------+-------+------+--------------+-------------------+--------+-------+----+----+----------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Faz a Leitura do arquivo CSV\"\"\"\n",
    "def reader_csv(spark: SparkSession) -> None:\n",
    "    df = spark.read.option(\"inferSchema\",True) \\\n",
    "                 .options(header='True', inferSchema='True', delimiter=';') \\\n",
    "                .csv(r\"/home/guilherme/docker-livy/code/apps/SEG_TIM_TREINAR_MODELO_V2_ABR_31-05_1_1.CSV\")\n",
    "    \n",
    "\n",
    "    print(df.show(10))\n",
    "\n",
    "    df.createOrReplaceTempView(\"ProdutosView\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executa_consulta_tabela1(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'starting', 'id': 0, 'kind': 'spark'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "host = 'http://localhost:8998'\n",
    "data = {'kind': 'spark'}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'state': u'starting', u'id': 0, u'kind': u'spark'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'idle', 'id': 0, 'kind': 'spark'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_url = host + r.headers['location']\n",
    "r = requests.get(session_url, headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'state': u'idle', u'id': 0, u'kind': u'spark'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': None, 'state': 'running', 'id': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements_url = session_url + '/statements'\n",
    "data = {'code': '1 + 1'}\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'output': None, u'state': u'running', u'id': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions fetched starting  0\n",
      "Number of session fetched 2\n",
      "The session 0 has a state idle \n",
      "The session 1 has a state idle \n"
     ]
    }
   ],
   "source": [
    "                            \n",
    "host = \"http://localhost:8998\"\n",
    "\n",
    "#Documentação https://livy.incubator.apache.org/docs/latest/rest-api.html#session\n",
    "\n",
    "\n",
    "sessions_url = host + \"/sessions\"\n",
    "\n",
    "\n",
    "auth = HTTPBasicAuth(\"***\", \"***\")\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "data = {\n",
    "        'from': 0, \n",
    "        'size': 10\n",
    "       }\n",
    "r = requests.get(sessions_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "\n",
    "response_body = r.json()\n",
    "\n",
    "print(\"Sessions fetched starting \", response_body['from'])\n",
    "print(\"Number of session fetched\", response_body['total'])\n",
    "session_list = response_body['sessions']\n",
    "\n",
    "for session in session_list:\n",
    "    print(\"The session {0} has a state {1} \".format(session['id'],session['state']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session 2 created. Current state is starting\n"
     ]
    }
   ],
   "source": [
    "#Create a new Spark session\n",
    "data = {\n",
    "    'kind':'pyspark'\n",
    "}\n",
    "\n",
    "r = requests.post(sessions_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "response_body = r.json()\n",
    "\n",
    "session_id = response_body['id']\n",
    "session_state = response_body['state']\n",
    "print(\"Spark session {0} created. Current state is {1}\".format(session_id,session_state))\n",
    "created_session_url = r.headers['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_session_url http://localhost:8998/sessions/2\n",
      "{'appId': None,\n",
      " 'appInfo': {'driverLogUrl': None, 'sparkUiUrl': None},\n",
      " 'id': 2,\n",
      " 'kind': 'pyspark',\n",
      " 'log': ['23/06/21 04:58:30 INFO Utils: Successfully started service '\n",
      "         \"'org.apache.spark.network.netty.NettyBlockTransferService' on port \"\n",
      "         '40609.',\n",
      "         '23/06/21 04:58:30 INFO NettyBlockTransferService: Server created on '\n",
      "         '45dab089587a:40609',\n",
      "         '23/06/21 04:58:30 INFO BlockManager: Using '\n",
      "         'org.apache.spark.storage.RandomBlockReplicationPolicy for block '\n",
      "         'replication policy',\n",
      "         '23/06/21 04:58:30 INFO BlockManagerMaster: Registering BlockManager '\n",
      "         'BlockManagerId(driver, 45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO BlockManagerMasterEndpoint: Registering block '\n",
      "         'manager 45dab089587a:40609 with 366.3 MB RAM, BlockManagerId(driver, '\n",
      "         '45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO BlockManagerMaster: Registered BlockManager '\n",
      "         'BlockManagerId(driver, 45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO BlockManager: Initialized BlockManager: '\n",
      "         'BlockManagerId(driver, 45dab089587a, 40609, None)',\n",
      "         '23/06/21 04:58:30 INFO StandaloneSchedulerBackend: SchedulerBackend '\n",
      "         'is ready for scheduling beginning after reached '\n",
      "         'minRegisteredResourcesRatio: 0.0',\n",
      "         '23/06/21 04:58:30 INFO SparkEntries: Spark context finished '\n",
      "         'initialization in 695ms',\n",
      "         '23/06/21 04:58:30 INFO SparkEntries: Created Spark session.'],\n",
      " 'name': None,\n",
      " 'owner': None,\n",
      " 'proxyUser': None,\n",
      " 'state': 'idle'}\n"
     ]
    }
   ],
   "source": [
    "#Query information about the session we just created\n",
    "this_session_url = host + created_session_url\n",
    "print(\"this_session_url\", this_session_url)\n",
    "\n",
    "r = requests.get(this_session_url, headers=headers,auth=auth, verify=False)\n",
    "pprint.pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'http://localhost:8998/sessions/2/statements'\n",
      "Respone status code 201\n",
      "Response received is  {'id': 2, 'code': '11+11', 'state': 'waiting', 'output': None, 'progress': 0.0, 'started': 0, 'completed': 0}\n",
      "Poll URI is  /sessions/2/statements/2\n"
     ]
    }
   ],
   "source": [
    "#Execute code interactively in this session using session/<ID>/statements interface\n",
    "\n",
    "statements_url = this_session_url + \"/statements\"\n",
    "pprint.pprint(statements_url)\n",
    "\n",
    "data = {\n",
    "  'code': \"11+11\"\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "print(\"Respone status code\" ,r.status_code )\n",
    "print(\"Response received is \",r.json())\n",
    "print(\"Poll URI is \",r.headers['location'])\n",
    "#response_body = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monitoring url is  http://localhost:8998/sessions/2/statements/2\n",
      "Response status is  200\n",
      "{'code': '11+11',\n",
      " 'completed': 1687323629800,\n",
      " 'id': 2,\n",
      " 'output': {'data': {'text/plain': '22'}, 'execution_count': 2, 'status': 'ok'},\n",
      " 'progress': 1.0,\n",
      " 'started': 1687323629799,\n",
      " 'state': 'available'}\n",
      "Response received is  None\n"
     ]
    }
   ],
   "source": [
    "# Wait a while before executing this cell. Spark session start up takes time to run your code.\n",
    "specific_statement = host + r.headers['location']\n",
    "print(\"monitoring url is \", specific_statement)\n",
    "\n",
    "r = requests.get(specific_statement, headers=headers, auth=auth, verify=False)\n",
    "print(\"Response status is \",r.status_code)\n",
    "print(\"Response received is \", pprint.pprint(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Envia arquivos Python para executar no Cluster do Spark\n",
    "\n",
    "\n",
    "\n",
    "batch_url = host + \"/batches\"\n",
    "print(\"batch_url\", batch_url)\n",
    "\n",
    "data = {\n",
    "    'file' : '/home/guilherme/docker-livy/submit_teste_01.py'    \n",
    "}\n",
    "\n",
    "r = requests.post(batch_url, data=json.dumps(data), headers=headers, auth=auth, verify=False)\n",
    "returned_batch_url = r.headers\n",
    "print(\"Respone status code\" , r.status_code)\n",
    "print(\"Poll URI is \",returned_batch_url )\n",
    "print(\"Response is \", pprint.pprint(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check results of executed code\n",
    "specific_batch = host + returned_batch_url\n",
    "print(\"specific batch request \",specific_batch)\n",
    "\n",
    "r = requests.get(specific_batch,headers=headers, auth=auth, verify = False)\n",
    "print(\"Response status is \",r.status_code)\n",
    "print(\"Response received is \", pprint.pprint(r.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Envia Codigo Para Cluster\n",
    "data = {\n",
    "  'code': textwrap.dedent(\"\"\"\n",
    "    import random\n",
    "    NUM_SAMPLES = 100000\n",
    "    def sample(p):\n",
    "      x, y = random.random(), random.random()\n",
    "      return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n",
    "    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "pprint.pprint(r.json())\n",
    "\n",
    "{u'id': 12,\n",
    "u'output': {u'data': {u'text/plain': u'Pi is roughly 3.136000'},\n",
    "            u'execution_count': 12,\n",
    "            u'status': u'ok'},\n",
    "u'state': u'running'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'kind': 'sparkr'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()\n",
    "\n",
    "{u'id': 1, u'state': u'idle'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  'code': textwrap.dedent(\"\"\"\n",
    "    n <- 100000\n",
    "    piFunc <- function(elem) {\n",
    "      rands <- runif(n = 2, min = -1, max = 1)\n",
    "      val <- ifelse((rands[1]^2 + rands[2]^2) < 1, 1.0, 0.0)\n",
    "      val\n",
    "    }\n",
    "    piFuncVec <- function(elems) {\n",
    "      message(length(elems))\n",
    "      rands1 <- runif(n = length(elems), min = -1, max = 1)\n",
    "      rands2 <- runif(n = length(elems), min = -1, max = 1)\n",
    "      val <- ifelse((rands1^2 + rands2^2) < 1, 1.0, 0.0)\n",
    "      sum(val)\n",
    "    }\n",
    "    rdd <- parallelize(sc, 1:n, slices)\n",
    "    count <- reduce(lapplyPartition(rdd, piFuncVec), sum)\n",
    "    cat(\"Pi is roughly\", 4.0 * count / n, \"\\n\")\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "pprint.pprint(r.json())\n",
    "\n",
    "{u'id': 12,\n",
    " u'output': {u'data': {u'text/plain': u'Pi is roughly 3.136000'},\n",
    "             u'execution_count': 12,\n",
    "             u'status': u'ok'},\n",
    " u'state': u'running'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Submit\"\"\"\n",
    "'''\n",
    "./bin/spark-submit \\\n",
    "  --master spark://207.184.161.138:7077 \\\n",
    "  /home/guilherme/docker-livy/submit_teste_01.py \\\n",
    "  1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Submit\"\"\"\n",
    "'''\n",
    "./bin/spark-submit \\\n",
    "  --master spark://207.184.161.138:7077 \\\n",
    "  /home/guilherme/docker-livy/submit_teste_02.py \\\n",
    "  1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
